{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc09c502-cfb8-4e0c-a64e-b0ed07cf2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_notebook_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:33:56.001940Z",
     "start_time": "2023-12-17T18:33:55.391849Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "from sklearn.datasets import fetch_openml\n",
    "import sys\n",
    "import numpy as np\n",
    "# np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed34aeb81e2009f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.280779Z",
     "start_time": "2023-12-17T18:33:56.000156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zivlazarov/miniforge3/envs/tensorflow-env/lib/python3.8/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724eddfe26c426a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.282264Z",
     "start_time": "2023-12-17T18:34:21.280158Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999605737a6fe6bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.288865Z",
     "start_time": "2023-12-17T18:34:21.281644Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e1678c18adcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.294835Z",
     "start_time": "2023-12-17T18:34:21.291806Z"
    }
   },
   "outputs": [],
   "source": [
    "X.iloc[0, -3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e57da071e01c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.301382Z",
     "start_time": "2023-12-17T18:34:21.294945Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting pixel values to be in [0, 1] range\n",
    "# X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a9b8d0fde7928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.339751Z",
     "start_time": "2023-12-17T18:34:21.309783Z"
    }
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2da81eb41574cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.421059Z",
     "start_time": "2023-12-17T18:34:21.388938Z"
    }
   },
   "outputs": [],
   "source": [
    "# getting the correct shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b3eb9d0901970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.421116Z",
     "start_time": "2023-12-17T18:34:21.392008Z"
    }
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de14ccb9cebe95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.421176Z",
     "start_time": "2023-12-17T18:34:21.395704Z"
    }
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721417d9c407fd1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.421229Z",
     "start_time": "2023-12-17T18:34:21.398510Z"
    }
   },
   "outputs": [],
   "source": [
    "# working with integers instead of strings for convenience\n",
    "y = y.apply(lambda x : int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420c4660bb29b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.421582Z",
     "start_time": "2023-12-17T18:34:21.402882Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee707104-55ee-441c-b457-89bc8ed7f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X.to_numpy(), y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97dadba68536ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T18:34:21.856221Z",
     "start_time": "2023-12-17T18:34:21.405945Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing distribution of pixels for every digit\n",
    "fig, ax = plt.subplots(5, 2, figsize=(10, 10))\n",
    "fig.tight_layout()\n",
    "for i in range(len(np.unique(y))):\n",
    "    # not taking the bias column into consideration\n",
    "    ax[i//2, i%2].hist(X[np.where(y == i)][:, 1:])\n",
    "    ax[i//2, i%2].set_title(f'Pixel count for {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc328b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting an idea of an image\n",
    "plt.imshow(X[0, :].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf41859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of each digit\n",
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae7297-220d-4570-9b27-0de30754e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the bias column as the first column\n",
    "X = np.concatenate((np.ones(shape=(X.shape[0],))[:, np.newaxis], X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e7bc1-29f4-459a-ac0e-e6300394c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bf6b9-fe71-4453-800f-93e243bfa3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f186f985d6589d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:32:36.918902Z",
     "start_time": "2023-12-17T19:32:36.910796Z"
    }
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, num_features, classes, epochs=1):\n",
    "        # num_features includes the extra bias column with the dataset so we'll add it\n",
    "        self.num_features = num_features\n",
    "        # values of the multi-class labels\n",
    "        self.classes = classes\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.num_samples = 0\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # initializing the weights vectors for every class with values [-1, 1]\n",
    "        self.weights = np.random.uniform(low=-1, high=1, size=(self.num_classes, self.num_features))\n",
    "        # weight vectors for pocketing\n",
    "        self.final_weights = None\n",
    "        # determined by the size of samples\n",
    "        self.labels = None\n",
    "                  \n",
    "    def init_multi_class_labels(self, y_train, negative_value=-1):\n",
    "        self.num_samples = y_train.shape[0]\n",
    "        # initializing the perceptron training labels\n",
    "        self.labels = np.ones(shape=(self.num_classes, self.num_samples))\n",
    "        # setting labels' values based on every class in the training data\n",
    "        for class_idx in range(self.num_classes):\n",
    "            self.labels[class_idx, np.where(y_train != class_idx)[0]] = negative_value\n",
    "\n",
    "    def fit(self, X_train, y_train, visualize=True):\n",
    "        self.init_multi_class_labels(y_train)\n",
    "        # defining error value for each class for pocket algorithm\n",
    "        min_errors = np.full(shape=(self.num_classes,), fill_value=np.inf)\n",
    "        \n",
    "        '''\n",
    "        choosing 16% (arbitrary decision) of the dataset as random samples to calculate errors,\n",
    "        as we can see from the above cells, the number of occurrences of each digit is \n",
    "        distributed almost uniformly, so getting 16% of the samples can distribute the amount of each\n",
    "        digit in the random sample almost evenly for more general optimization\n",
    "        '''\n",
    "        num_random_samples = int(0.16 * self.num_samples)\n",
    "        # initializing the final_weights here for easier usage with inheritance\n",
    "        self.final_weights = np.copy(self.weights)\n",
    "        visualization_iteration_skip = 10\n",
    "        # error values for every 10th iteration (/sample) for the visualization\n",
    "        iterations_errors = np.full(shape=(self.num_samples//visualization_iteration_skip, self.num_classes), fill_value=np.inf)\n",
    "        # traversing the dataset (per sample approach)\n",
    "        for epoch in range(self.epochs):\n",
    "            for t in range(self.num_samples):\n",
    "                # predicting the current sample with every weight vector using the np.sign method\n",
    "                # because probability of the dot product to be exactly 0 is very low, and if so, we'll count predictions on the hyperplane\n",
    "                # to be negative\n",
    "                y_preds = np.sign(self.weights.dot(X_train[t]))\n",
    "\n",
    "                # dividing classes based on their predictions\n",
    "                misclassified_classes_indexes = np.where(y_preds != self.labels[:, t])\n",
    "                # correcting misclassified weight vectors by multiplying the true sign value by the current sample for each weight vector\n",
    "                self.weights[misclassified_classes_indexes] += \\\n",
    "                    self.labels[misclassified_classes_indexes, t][0][:, np.newaxis] * X_train[t, :]\n",
    "                    \n",
    "                # pocketing every 10 samples\n",
    "                if t % visualization_iteration_skip == 0:\n",
    "                    # getting random indexes for the sampling\n",
    "                    random_samples_indexes = np.random.choice(self.num_samples, num_random_samples)\n",
    "\n",
    "                    # calculating the errors from the random samples based on the improved vectors\n",
    "                    y_preds_unsigned = self.weights.dot(X_train[random_samples_indexes, :].T)\n",
    "                    errors_t = np.sum(\n",
    "                        np.sign(y_preds_unsigned) != self.labels[:, random_samples_indexes],\n",
    "                        axis=1) / num_random_samples\n",
    "\n",
    "                    if visualize:\n",
    "                        iterations_errors[t//visualization_iteration_skip] = np.copy(errors_t)\n",
    "\n",
    "                    # pocketing the improved weight vectors and updating the min errors\n",
    "                    to_improve_classes_indexes = np.where(errors_t < min_errors)[0]\n",
    "                    \n",
    "                    self.final_weights[to_improve_classes_indexes] = \\\n",
    "                        np.copy(self.weights[to_improve_classes_indexes])\n",
    "                    \n",
    "                    min_errors[to_improve_classes_indexes] = errors_t[to_improve_classes_indexes]\n",
    "                    \n",
    "        # visualizing the errors for each class\n",
    "        if visualize:\n",
    "            fig, ax = plt.subplots(self.num_classes//2, 2, figsize=(10, 10))\n",
    "            fig.tight_layout()\n",
    "            x_axis = [epoch*visualization_iteration_skip for epoch in range(iterations_errors.shape[0])]\n",
    "            for i in range(iterations_errors.shape[1]):\n",
    "                ax[i//2, i%2].plot(x_axis, iterations_errors[:, i])\n",
    "                ax[i//2, i%2].set_title(f'Error for class {i}')\n",
    "            # setting the max value of the y axis to be 0.5 for better readability\n",
    "            plt.setp(ax, xlim=(0, self.num_samples), ylim=(0, 0.5))\n",
    "            plt.show()\n",
    "        \n",
    "        print(min_errors)\n",
    "        # setting the trained weight vectors\n",
    "        self.weights = np.copy(self.final_weights)\n",
    "                        \n",
    "    def predict(self, X_test):\n",
    "        return np.argmax(X_test.dot(self.weights.T), axis=1)\n",
    "    \n",
    "    def accuracy(self, predictions, y_test):\n",
    "        return np.sum(predictions == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913eff25e86ee90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:32:42.414465Z",
     "start_time": "2023-12-17T19:32:42.396502Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "num_train_samples = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630136d790aaff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:44:56.213161Z",
     "start_time": "2023-12-17T19:44:55.925796Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=num_train_samples/len(X), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4e68e0e2ae0a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:44:56.349486Z",
     "start_time": "2023-12-17T19:44:56.337730Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f265eab9140f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:44:57.897375Z",
     "start_time": "2023-12-17T19:44:57.885417Z"
    }
   },
   "outputs": [],
   "source": [
    "#perceptron = Perceptron(num_features=X_train.shape[1], classes=np.unique(y), epochs=1)\n",
    "from .. import models\n",
    "perceptron = models.Perceptron(num_features=X_train.shape[1], classes=np.unique(y), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c4a143d748919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:53:28.602731Z",
     "start_time": "2023-12-17T19:45:00.160605Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "perceptron.fit(X_train, y_train)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb5548-f480-4399-aaff-46a74d653f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759fa23-b671-48f8-9126-a3c37fe94f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = perceptron.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a38dd2c8ac96170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:53:28.802065Z",
     "start_time": "2023-12-17T19:53:28.797871Z"
    }
   },
   "outputs": [],
   "source": [
    "perceptron.accuracy(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8d6bd-8275-470f-85ef-737e4caa2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def visualize_metrics(class_idx, X_test, y_test, model):\n",
    "    if class_idx == -1:\n",
    "        predictions = model.predict(X_test)\n",
    "        confusion_matrix = np.zeros(shape=(model.num_classes, model.num_classes))\n",
    "        # calculating the confusion matrix\n",
    "        for idx in range(len(y_test)):\n",
    "            y_test_idx = y_test[idx]\n",
    "            preds_idx = predictions[idx]\n",
    "            confusion_matrix[y_test_idx, preds_idx] += 1\n",
    "        # displaying the confusion matrix with a heatmap\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt='.4g')\n",
    "        plt.ylabel('Y True')\n",
    "        plt.xlabel('Y Pred')\n",
    "        plt.title(f'Multi Class Perceptron Confusion Matrix\\nAccuracy: {round(np.sum(predictions == y_test) / len(y_test), 3)}')\n",
    "    else:\n",
    "        # creating the grid for the classes\n",
    "        fig, ax = plt.subplots(model.num_classes//2, 2, figsize=(10, 10), constrained_layout=True)\n",
    "        # fig.tight_layout()\n",
    "        for i in range(perceptron.num_classes):\n",
    "            class_sign_y = np.ones(shape=(len(y_test), ))\n",
    "            class_sign_y[np.where(y_test != model.classes[i])] = -1\n",
    "\n",
    "            predictions = np.sign(X_test.dot(model.weights[i]))\n",
    "            \n",
    "            # predicted class_idx correctly\n",
    "            tp = len(class_sign_y[(class_sign_y == predictions) & (class_sign_y == 1)])\n",
    "            # predicted not class_idx correctly\n",
    "            tn = len(class_sign_y[(class_sign_y == predictions) & (class_sign_y == -1)])\n",
    "            # predicted class_idx incorrectly\n",
    "            fp = len(class_sign_y[(class_sign_y != predictions) & (class_sign_y == -1)])\n",
    "            # predicted not class_idx incorrectly\n",
    "            fn = len(class_sign_y[(class_sign_y != predictions) & (class_sign_y == 1)])\n",
    "\n",
    "            accuracy = (tp + tn) / len(y_test)\n",
    "            sensitivity = tp / (tp + fn)\n",
    "            selectivity = tn / (tn + fp)\n",
    "            \n",
    "            confusion_matrix = np.zeros(shape=(2, 2))\n",
    "            confusion_matrix[0, 0] = tp\n",
    "            confusion_matrix[0, 1] = fn\n",
    "            confusion_matrix[1, 0] = fp\n",
    "            confusion_matrix[1, 1] = tn\n",
    "            \n",
    "            sns.heatmap(confusion_matrix, ax=ax[i//2, i%2], annot=True, fmt='.4g')\n",
    "            ax[i//2, i%2].set_title(f'Confusion Matrix for Class {model.classes[i]}\\n' + \\\n",
    "            f'Acc: {round(accuracy, 3)} | Sen: {round(sensitivity, 3)} | Sel: {round(selectivity, 3)}')\n",
    "            ax[i//2, i%2].set_ylabel('Y True')\n",
    "            ax[i//2, i%2].set_xlabel('Y Pred')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3c029-c7ea-4064-ad4a-eda0f0c8ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_metrics(-1, X_test, y_test, perceptron)\n",
    "visualize_metrics(0, X_test, y_test, perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe15e6-5bd6-40b9-953e-399f9501d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We can see from the metrics that out selectivity for each digit is very high, and our sensitivity is a little lower, which means our binary\n",
    "classifiers are not classifying false positives in large quantities, and some are classifying false negatives more than they \"should\".\n",
    "This gives us more confidence in the classifiers for not predicting positively incorrectly, but they will predict more negatives in such\n",
    "matter.\n",
    "In terms of accuracy, the binary classifiers are very accurate.\n",
    "In respect to the multi class perceptron, we can see from the loss functions during training that 0,1,4,6,7 digits are more stable in terms\n",
    "of the loss values, and somehow their loss values are converging towards 0, which the other digits are more noisy and are less\n",
    "confident during training.\n",
    "My assumption is that more strong pixel values in the samples are affecting this scenario, as 2,3,5,8,9 are digits which require more\n",
    "pixels to show.\n",
    "For the training itself, I went for the more efficient-focused implementation, using the fact that the digits in the dataset are \n",
    "distributed almost uniformly which makes it easy for us to random-sample the training data to calculate our loss function with\n",
    "confidence as we can assume the samples will also distribute almost uniformly in the calculation. This approach led me to calculate the\n",
    "loss function and pocket the relevant weights vectors accordingly every 10 iterations to save valuable time and resources during\n",
    "training without sacrificing the correctness of the algorithm.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c54724-ed22-46e6-927c-40bc01f575ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarizing the features' values\n",
    "# X_train[:, 1:], X_test[:, 1:] = X_train[:, 1:] / 255.0, X_test[:, 1:] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ba0c9-cdcc-4e3d-80e0-301c1b5fc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    # normalize values to avoid over/underflow of the values (s can't be the zero vector)\n",
    "    mean = np.mean(s, keepdims=True)\n",
    "    std = np.std(s, keepdims=True)\n",
    "    z = (s - mean) / std\n",
    "    z = np.exp(z)\n",
    "    return z / np.sum(z, axis=0)\n",
    "\n",
    "# inheriting from the perceptron class to use the predict and metrics methods\n",
    "class LogisticRegression(Perceptron):\n",
    "\n",
    "    def __init__(self, num_features, classes, learning_rate=0.01, epochs=1):\n",
    "        # initializing variables like the perceptron\n",
    "        super().__init__(num_features=num_features, classes=classes, epochs=epochs)\n",
    "        self.lr = learning_rate\n",
    "        # weights vectors' components are still uniformly distributes between -1 and 1\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # setting the negative value to be zero for the indicator in the gradient calculation\n",
    "        super().init_multi_class_labels(y_train=y_train, negative_value=0)\n",
    "\n",
    "        # need gradients to be of shape (self.num_classes, self.num_features)\n",
    "        # where gradients[i] = sum((softmax(w_i, x_n) - I[y_n = i]) * x_n)\n",
    "        for epoch in range(self.epochs):\n",
    "            # shape = (num_classes x num_samples)\n",
    "            # row i refers to the exponent of w_i with all the samples\n",
    "            weights_X_exp = self.weights @ X_train.T\n",
    "\n",
    "            '''\n",
    "            1. self.labels is the indicator in the gradient.\n",
    "            2. multiplying every result (i.e softmax - indicator) by the matching sample\n",
    "            needed to add a new dimension to multiply each result by the relevant sample for each of the weight vectors\n",
    "            '''\n",
    "            gradients = np.sum((softmax(weights_X_exp) - self.labels)[:, :, np.newaxis] * X_train, axis=1)\n",
    "            \n",
    "            # making the weight vectors to go in the opposite direction of the matching gradient self.lr \"steps\"\n",
    "            self.weights -= self.lr * gradients\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return np.argmax(softmax(X_test @ self.weights.T), axis=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee9fbb-3d46-469b-af00-ce8729632783",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(X.shape[1], np.unique(y), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c436dc-fc0b-42e0-ab17-daa07b67c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee4b1d-4003-42d4-aee3-a5b78957798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logistic_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4200c5-c0d2-4931-9df9-e45c0f2efa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40619d-a912-45c6-b125-b2a2f728783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.accuracy(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ff209-3453-4c3a-8f7f-7548cd67c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time() - start_notebook_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91595bf0-320d-4ae4-9e7d-853bb50c9e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
